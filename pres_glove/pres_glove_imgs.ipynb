{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b5119e-c9cd-46e3-9c34-06b0a3d5a76c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Presentación GloVe\n",
    "- Fernanda Rubio\n",
    "- Roberto Pérez \n",
    "- Víctor Rivera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2a541-a81e-48c2-bff6-7f77e0a02521",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8684dd-605e-4f30-8198-cce51f81c75c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Existen dos familias de algoritmos para \"aprender\" vectores de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31af198-7f23-488f-ab12-1942851c87eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- **Global Matrix Factorization** (e.g. LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9dbd7c-3f57-4630-9164-3c50f72a3e6e",
   "metadata": {},
   "source": [
    "    Ventaja: Aprovechan información estadística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71a7a2-6ca1-4ded-b8d9-28801114983b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- **Local Context Widow** (e.g. skip-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22d125-8cde-4b78-94a6-d1a027402020",
   "metadata": {},
   "source": [
    "    Ventaja: Buen desempeño encontrando analogías de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de540c68-c5b1-412d-acba-01f2b018dd70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**GloVe** (Global Vectors) combina las ventajas de las 2 familias de modelos   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a7552-ead0-49a0-b3f1-8fcbac918867",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b058d23-56fe-4001-9693-4f7a41023e1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "GloVe es un algoritmo *no supervisado* para obtener representaciones vectoriales de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4eadcd-1e9a-4e45-89c7-fa184929178c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Sea:\n",
    "- $X$ la matriz de coocurrencia.\n",
    "- $X_{i,j}$ el número de veces la palabra j ocurre en el contexto de la palabra i.\n",
    "- $X_{i} = \\sum_{k}X_{i,k}$ el número de veces que la palabra i ocurre en el contexto de cualquier palabra.\n",
    "- $P_{i,j} = P(j|i) = \\frac{X_{i,k}}{X_{j,k}}$ la probabilidad de que j aparezca en el contexto de i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e67711-0241-4e0f-9415-8ec93a4d2868",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Para 2 palabras $i$, $j$ y un set de palabras $k$ de contexto, su relación se estudia por el ratio de sus coocurrencias:\n",
    "\n",
    "  * para palabras relacionadas a i pero no a j $\\frac{P_{i,k}}{P_{j,k}}$ será grande.\n",
    "  * para palabras relacionadas a j pero no a i  $\\frac{P_{i,k}}{P_{j,k}}$ será pequeño.\n",
    "  * para palabras relacionadas por igual a i y j o no relacionadas $\\frac{P_{i,k}}{P_{j,k}}$ será cercano a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c5e5fb-30d9-4a44-8869-260f9a0f9b13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1hisLB3xsrAkDnkz2J15M3O-JiYE_q39B\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48c3db-9cbc-4571-9770-62e3482cb6f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Comparar estos ratios nos permite saber qué palabras son relevantes y a discriminar para qué palabra son relevantes. Entonces el principio de GloVe es una función que depende de 3 palabras i, j y k:\n",
    "$$F(w_{i}, w_{j}, \\hat{w}_{k})=\\frac{P_{i,k}}{P_{j,k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b1e5d-0c42-4cfe-ba95-eb85a488d647",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "donde: \n",
    "- $w_{i}, w_{j}$ son vectores de palabras\n",
    "- $\\hat{w}_{k}$ vectores de palabras de contexto\n",
    "- $F$ puede ser cualquier función pero nos gustaría que capturara más información de $\\frac{P_{i,k}}{P_{j,k}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb76dde-c899-48c4-99ed-edf237d552b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Planteando el problema como uno de mínimos cuadrados, el objetivo GloVe se vuelve minimizar la siguiente función de pérdida:\n",
    "$$J=\\sum_{i,j=1}^{V}f(X_{ij})(w_{i}^{t}\\hat{w}_{k}+b_{i}+\\hat{b}_{k}-ln(X_{ik}))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3b371-e8d9-4284-acd0-793b4d7f31c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Ejemplo aplicado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6cb852-07e1-4482-be48-6231116881a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Cargamos una muestra de artículos de Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada6197-c48a-46cf-bb22-06cffe4ec45a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_corp <- quanteda.corpora::download(\n",
    "    url = \"https://www.dropbox.com/s/9mubqwpgls3qi9t/data_corpus_wiki.rds?dl=1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea725f6-834c-4a02-983e-262d62ed8649",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Creación de vocabulario del que se aprenderán los vectores de palabras:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b02fe-f123-4cd9-8012-12cd36855d84",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "1: Tokenizar el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c42079-8af7-467a-8d8a-8ce8930d633b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_toks <- tokens(wiki_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfcf5c-9137-4296-8ff9-b11027d32689",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "2: Extración de los features que suceden 5 veces o más:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c643c3-fa61-4f6e-9433-12dfb7de38c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats <- dfm(wiki_toks, verbose = TRUE) %>%\n",
    "    dfm_trim(min_termfreq = 5) %>%\n",
    "    featnames()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed6301-fce5-4b81-bddd-71e1d33c509b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1Xd6o5c-VbRw3qbJeAWuKbIHEwZAiTUme\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4becabfc-fdf8-4f96-8cc5-e48432a4671e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_toks <- tokens_select(wiki_toks, feats, padding = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27cf47d-de3b-4fae-a15c-780f900e7adf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Contrucción de la matriz de concurrencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8595b1-635e-43bd-9889-88ac18e4c00c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_fcm <- fcm(wiki_toks, context = \"window\", count = \"weighted\", weights = 1 / (1:5), tri = TRUE)\n",
    "wiki_fcm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1db55a-80a4-4a50-bdaa-33a6edd2ef5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Modelo GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b698c0-488c-4c30-97c6-3584fcc4a256",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5d634-d19c-4fb6-a6ae-094767807716",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "glove <- GlobalVectors$new(rank = 50, x_max = 10)\n",
    "wv_main <- glove$fit_transform(wiki_fcm, n_iter = 10,\n",
    "                               convergence_tol = 0.01, n_threads = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b5bce-4b9c-4233-845e-76e4802beb35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1rWhZehCPj-fkioj39sv7qEbOsD5hPcds\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e128cb-b6ff-4b09-8f37-552da20e47ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Suma de palabras con el contexto para mejorar la precisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c216c-5b37-4485-aa3b-bbb6f5ff3359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wv_context <- glove$components\n",
    "word_vectors <- wv_main + t(wv_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92524a42-5dcd-486c-91df-b36b878d2c31",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Entrenar word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92089e16-7051-4f89-bacc-bd83c849399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizar <- function(texto, vocab = NULL){\n",
    "  # minúsculas\n",
    "  texto <- tolower(texto)\n",
    "  # varios ajustes\n",
    "  texto <- gsub(\"\\\\s+\", \" \", texto)\n",
    "  texto <- gsub(\"\\\\.[^0-9]\", \" _punto_ \", texto)\n",
    "  texto <- gsub(\" _s_ $\", \"\", texto)\n",
    "  texto <- gsub(\"\\\\.\", \" _punto_ \", texto)\n",
    "  texto <- gsub(\"[«»¡!¿?-]\", \"\", texto) \n",
    "  texto <- gsub(\";\", \" _punto_coma_ \", texto) \n",
    "  texto <- gsub(\"\\\\:\", \" _dos_puntos_ \", texto) \n",
    "  texto <- gsub(\"\\\\,[^0-9]\", \" _coma_ \", texto)\n",
    "  texto <- gsub(\"\\\\s+\", \" \", texto)\n",
    "  texto\n",
    "}\n",
    "wiki_df <- tibble(txt = wiki_corp) %>%\n",
    "                mutate(id = row_number()) %>%\n",
    "                mutate(txt = normalizar(txt))\n",
    "\n",
    "if(!file.exists('./salidas/wiki_w2v.txt')){\n",
    "  tmp <- tempfile()\n",
    "  # tokenización\n",
    "  write_lines(wiki_df$txt,  tmp)\n",
    "  prep <- prep_word2vec(tmp, \n",
    "          destination = './salidas/wiki_w2v.txt', bundle_ngrams = 2)\n",
    "} \n",
    "\n",
    "if (!file.exists(\"./salidas/wiki_vectors.bin\")) {\n",
    "  model_w2v <- train_word2vec(\"./salidas/wiki_w2v.txt\", \n",
    "          \"./salidas/wiki_vectors.bin\",\n",
    "          vectors = 100, threads = 4, window = 5, cbow = 0,  \n",
    "          iter = 5, negative_samples = 20, min_count = 5) \n",
    "} else {\n",
    "  model_w2v <- read.vectors(\"./salidas/wiki_vectors.bin\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1537eb9-8f10-4223-b92e-6b2e5b981b61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c183ec-1bfd-48b2-946b-7d24ed842bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data <- read_delim(\"words_for_testing.csv\", delim = \" \", col_names = F) %>%\n",
    "  rename(group = X5, word_to_predict = X3) %>%\n",
    "  select(X1, X2, X4, word_to_predict, group) %>%\n",
    "  mutate_all(tolower) %>%\n",
    "\n",
    "filter(X1 %in% rownames(word_vectors), \n",
    "       X2 %in% rownames(word_vectors), \n",
    "       X4 %in% rownames(word_v ectors)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3573c-e142-4977-a550-b52410fee207",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1iAHMokUtnpo6pFxFe6EIJgahS_dSbVcK\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5126352-dc10-429c-8ab1-cf1093497ceb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Comparación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f517ef-26ea-4c54-9153-3bf406916155",
   "metadata": {},
   "source": [
    "Esta función calcula los vectores de diferencia y obtiene la palabra con mayor similitud para cada modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39c14d-d470-4d14-af7d-57206a77219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_comp <- function(test_data){ for (i in 1:nrow(test_data)) {\n",
    "    vec_glove <- word_vectors[test_data$X1[i], , drop = FALSE] -\n",
    "      word_vectors[test_data$X2[i], , drop = FALSE] +\n",
    "      word_vectors[test_data$X4[i], , drop = FALSE]\n",
    "    vec_w2v <- model_w2v[[test_data$X1[i]]] - model_w2v[[test_data$X2[i]]] +\n",
    "      model_w2v[[test_data$X4[i]]]\n",
    "    cos_sim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec_glove), method = \"cosine\"\n",
    ")\n",
    "    order_words <- head(sort(cos_sim[, 1], decreasing = TRUE), 5)\n",
    "    closest_word_glove <- order_words %>% as_tibble() %>% mutate(word = names(order_words)) %>%\n",
    "      filter(word != test_data$X4[i])\n",
    "    closest_word_w2v <- model_w2v %>% closest_to(vec_w2v, n = 5) %>%\n",
    "      filter(word != test_data$X4[i])\n",
    "    test_data$prediction_glove[i] <- closest_word_glove$word[1]\n",
    "    test_data$conf_glove[i] <- closest_word_glove$value[1]\n",
    "    test_data$prediction_w2v[i] <- closest_word_w2v$word[1]\n",
    "    test_data$conf_w2v[i] <- closest_word_w2v$`similarity to vec_w2v`[1]\n",
    "}\n",
    "test_data }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db47c6-53b2-4d6b-b795-659007ef9434",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Prueba con datos de familia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd6f7f-3cdd-48bc-b0ca-4c9bda49fe1e",
   "metadata": {},
   "source": [
    "Estos datos reflejan relaciones familiares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a2831-75b9-47cc-b253-8f40d50ed7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_test_data <- test_data %>%\n",
    "  filter(group == 'family') %>%\n",
    "  select(-group)\n",
    "head(fam_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825fe6aa-ac87-441e-b2e5-a320f1adf754",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1HKwv5uL8R0b8fvUfh8r6tK57mkZ4Il7N\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38bc7f-eb55-4cbe-aa48-22ba45576e46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Aplicamos la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2b26f-3ca1-48ee-9321-05b383a4f367",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fam_test_data <- fam_test_data %>%\n",
    "  mutate(prediction_glove = NA, conf_glove = NA, prediction_w2v = NA, conf_w2v = NA)\n",
    "fam_test_data <- similarity_comp(fam_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e77616-2fe0-45d2-adfa-3a94aaa3e3a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Vemos en cuántas ocasiones el modelo dio la palabra que estabamos buscando, en este caso el performance de GloVe es mejor que el de word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e3fa9-fb03-4a6b-99ee-b2f7e4e25eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_eval <- fam_test_data %>%\n",
    "  mutate(equal_glove = if_else(word_to_predict == prediction_glove, 1, 0),\n",
    "         equal_w2v = if_else(word_to_predict == prediction_w2v, 1, 0))\n",
    "fam_eval %>% filter(equal_glove == 1) %>% head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282a76e-f3c1-4ee0-b193-9dd7c3315e8d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=13oW_DX6tVDGWAAgFeJ-1zQGS8Gb0XoVB\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffdde04-73a1-4b35-8999-0df84cec44fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fam_eval %>%\n",
    "  summarise(sum(equal_glove), sum(equal_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1088c7-c762-4741-9231-9aa4e3615d0f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1nQhyUQoZ-sL4BwZs8UYROvO8uNr855T7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f698d4-31de-4e0e-9433-e714244718a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Prueba con datos de capitales del mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27645020-1b6c-46ee-960f-79970999f987",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_test_data <- test_data %>%\n",
    "  filter(group == 'capital-world') %>%\n",
    "  select(-group)\n",
    "head(country_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b07a9-464e-492f-89ac-a5c362e65721",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1PWt1sxs3jMKS_MZPUNELtfeBVxUJoJDw\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f97ff1-f73a-4652-8617-38b88daa37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_test_data <- country_test_data %>%\n",
    "  mutate(prediction_glove = NA, conf_glove = NA, prediction_w2v = NA, conf_w2v = NA)\n",
    "country_test_data <- similarity_comp(country_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4de746-62ac-40e1-86e4-4cf971e1bb4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Vemos en cuántas ocasiones el modelo dio la palabra que estabamos buscando, en este caso el performance de GloVe es mejor que el de word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8eaa9d-02bc-4be4-9c75-7552fa231188",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_eval <- country_test_data %>%\n",
    "  mutate(equal_glove = if_else(word_to_predict == prediction_glove, 1, 0),\n",
    "         equal_w2v = if_else(word_to_predict == prediction_w2v, 1, 0))\n",
    "country_eval %>% filter(equal_glove == 1) %>% head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015e367-c72d-457e-bfed-a6b0bb772d38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1Gztn0C2HwdWgIXq2Hv3fxYRsJicTnDcB\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b138476-8d3e-4850-b3b8-0dae343b0354",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_eval %>%\n",
    " summarise(sum(equal_glove), sum(equal_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3889e-19e2-430e-b248-96ed49a834bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1wptwnowiykhxQRl15bbCN19S1rg-yobp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cbf1b-aaf3-429f-86f3-14c35e8b9335",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Gracias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
